{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spotify Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM2lFnHtgTGdw0OGHG2COCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbendada/playlist-completion-challenge/blob/master/Spotify_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSRoBsIt0zoh",
        "outputId": "757a526a-62b0-4a73-fa4e-5d83d06eaf5d"
      },
      "source": [
        "from google.colab import drive\n",
        "from math import floor, log2, ceil\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, lil_matrix, load_npz, save_npz\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "notebooks_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/', notebooks_path)\n",
        "sys.path.insert(0,notebooks_path)\n",
        "#!pip uninstall implicit\n",
        "#!pip install --target=$nb_path implicit\n",
        "MPD_PATH = \"./drive/MyDrive/Colab Notebooks/million_playlist_dataset\"\n",
        "os.chdir(MPD_PATH)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o4Qg-iNjpb7"
      },
      "source": [
        "from src.gated_cnn import GatedCNN\n",
        "from src.evaluator import Evaluator\n",
        "from src.data_manager import DataManager\n",
        "from src.model import ChartsModel, CompletionModel, MatrixFactorizationModel, ItemItemModel, UserUserModel, EnsembleModel\n",
        "\n",
        "data_manager = DataManager()\n",
        "test_evaluator = Evaluator(data_manager)\n",
        "#charts_model = ChartsModel(data_manager)\n",
        "#mf_model = MatrixFactorizationModel(emb_size=256, data_manager=data_manager)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anyxDa--c2b1"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoZiTBBvEZS6"
      },
      "source": [
        "user_user = UserUserModel(data_manager, retrain=False)\n",
        "item_item = ItemItemModel(data_manager, retrain=False)\n",
        "mf_model = MatrixFactorizationModel(emb_size=256, data_manager=data_manager)\n",
        "mf_model.prepare_item_factors()\n",
        "em = EnsembleModel(data_manager, models=[user_user, item_item, mf_model])\n",
        "#combinations, recos, ndcgs = em.find_optimal_weights(test_evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ea9MTwi0jV"
      },
      "source": [
        "class DataManager():\n",
        "  def __init__(self, foldername = \"data/processed_data\", test_size=1000, min_songs_test=5, resplit=False):\n",
        "    self.foldername = foldername \n",
        "    self.test_size = test_size\n",
        "    self.min_songs_test = min_songs_test\n",
        "    self.load_playlist_track()\n",
        "    self.load_track_info()\n",
        "    self.n_playlists = self.playlist_track.shape[0]\n",
        "    self.n_tracks = self.playlist_track.shape[1]\n",
        "    self.train_indices = self.get_indices(\"train\", resplit=resplit)\n",
        "    self.val_indices = self.get_indices(\"val\")\n",
        "    self.binary_val_set, self.val_ground_truth = self.get_ground_truth(\"val\")\n",
        "    self.binary_train_set = self.get_train_set()\n",
        "    del self.playlist_track\n",
        "    self.prepare_charts()\n",
        "\n",
        "  def load_playlist_track(self):\n",
        "    self.playlist_track = load_npz(\"%s/playlist_track.npz\" % self.foldername)\n",
        "  \n",
        "  def load_track_info(self):\n",
        "    with open(\"%s/track_info.json\" % self.foldername) as f :\n",
        "      self.tracks_info = json.load(f)\n",
        "  \n",
        "  def prepare_charts(self):\n",
        "    self.ordered_tracks = [e[0] for e in sorted({v[\"id\"]:v[\"count\"] for k,v in self.tracks_info.items()}.items(), key=operator.itemgetter(1), reverse=True)]\n",
        "    self.ordered_tracks.insert(0, self.n_tracks)\n",
        "    self.tracks_rank = np.zeros(self.n_tracks + 1, dtype=np.int32)\n",
        "    for i,t in enumerate(self.ordered_tracks):\n",
        "      self.tracks_rank[t] = i\n",
        "    self.ordered_tracks = np.array(self.ordered_tracks)\n",
        "  \n",
        "  def split_sets(self):\n",
        "    playlist_track_csc = self.playlist_track.tocsc()\n",
        "    candidate_indices = np.random.choice(list(set(playlist_track_csc.indices[playlist_track_csc.data > 2*self.min_songs_test])), 2*self.test_size, replace = False) # find all playlists that have at least 10 songs\n",
        "  \n",
        "    tmp_test_indices = candidate_indices[:self.test_size] \n",
        "    tmp_val_indices = candidate_indices[self.test_size:]\n",
        "    train_indices = [i for i in range(self.n_playlists) if i not in candidate_indices]\n",
        "\n",
        "    val_indices = self.get_valid_playlists(train_indices, tmp_val_indices)\n",
        "    test_indices = self.get_valid_playlists(train_indices, tmp_test_indices)\n",
        "    np.save('%s/train_indices' % (self.foldername), train_indices)\n",
        "    np.save('%s/val_indices' % (self.foldername), val_indices)\n",
        "    np.save('%s/test_indices' % (self.foldername), test_indices)\n",
        "\n",
        "  def get_indices(self, set_name, resplit = False):\n",
        "    if resplit:\n",
        "      self.split_sets()\n",
        "    return np.load(\"%s/%s_indices.npy\" % (self.foldername, set_name))\n",
        "      \n",
        "  def get_valid_playlists(self, train_indices, test_indices):\n",
        "    # removes playlists in test set that have songs with no occurence in the train set\n",
        "    train_tracks = set(self.playlist_track[train_indices].indices)\n",
        "    test_tracks = set(self.playlist_track[test_indices].indices)\n",
        "    test_size = len(test_indices)\n",
        "    invalid_tracks = test_tracks - train_tracks\n",
        "    invalid_positions = set()\n",
        "    v = self.playlist_track[test_indices].tocsc()\n",
        "    for i in invalid_tracks:\n",
        "      invalid_positions = invalid_positions.union(set(v.indices[v.indptr[i]:v.indptr[i+1]]))\n",
        "    valid_positions = np.array(sorted([p for p in range(test_size) if p not in invalid_positions]))\n",
        "    return test_indices[valid_positions]\n",
        "  \n",
        "  def get_ground_truth(self, set_name, binary = True, resplit=False):\n",
        "    indices = self.get_indices(set_name, resplit)\n",
        "    data = self.playlist_track[indices]\n",
        "    ground_truth_array = data.multiply(data > self.min_songs_test)\n",
        "    start_data = data - ground_truth_array\n",
        "    if binary:\n",
        "      start_data = 1 * (start_data > 0)\n",
        "    ground_truth_list = []\n",
        "    for i in range(data.shape[0]):\n",
        "      ground_truth_list.append(set(ground_truth_array.indices[ground_truth_array.indptr[i]:ground_truth_array.indptr[i+1]]))\n",
        "    return start_data, ground_truth_list\n",
        "\n",
        "  def get_train_set(self, binary = True, resplit=False):\n",
        "    train_indices = self.get_indices(\"train\", resplit)\n",
        "    train_set = self.playlist_track[train_indices]\n",
        "    if binary :\n",
        "      train_set = 1 * (train_set > 0)\n",
        "    return train_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShYeYTxK3GDs"
      },
      "source": [
        "# Next step : https://github.com/jojonki/Gated-Convolutional-Networks "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iljq5a46vEs"
      },
      "source": [
        "# Deep Convolutionnal Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWiam7bkIbm2"
      },
      "source": [
        "# Input data is in the form of a csv, one row per playlist, list of ids separated by commas\n",
        "# Prerequisite : order tracks by popularity for adaptive softmax. tracks_rank[track_id] == track_rank / ordered_tracks[tracks_rank] == track_id\n",
        "#\n",
        "# \n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7d_flLUIkVR"
      },
      "source": [
        "mid_dim=128\n",
        "n_layers        = 8\n",
        "kernel          = (3, mid_dim)\n",
        "conv_size = 128\n",
        "hidden_size = 64\n",
        "padding = (1, 0)\n",
        "res_block_count = 5\n",
        "batch_size      = 256\n",
        "seq_len = 6 # 5 + 1 to predict\n",
        "cutoffs = [1000, 4000, 15000, 65000]\n",
        "end_of_seq = data_manager.n_tracks\n",
        "channels = mid_dim\n",
        "k=4 #kernel_width\n",
        "test_batch_size = 32\n",
        "sequential_data_file = \"data/playlists_seq.csv\"\n",
        "train_indices = data_manager.train_indices\n",
        "val_indices = data_manager.val_indices\n",
        "item_factors_path = 'data/models/item_factors_frequency_reordered.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaJ_FElnRQyD"
      },
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "# Training : for each row, split text, convert to int. If sequence is shorter than 5, select all sequence + padding at the end. Otherwise, randomly select a subsequence of 5 consecutive tracks\n",
        "class SequentialTrainDataset(Dataset):\n",
        "    def __init__(self, filename, train_indices, sample_size, index_encoder):\n",
        "        self.sample_size = sample_size\n",
        "        self.index_encoder = index_encoder\n",
        "        self.data = pd.read_csv(filename, delimiter ='\\t',  header=None, names = ['tracks']).iloc[train_indices]\n",
        "        self.data['tracks'] = self.data['tracks'].apply(lambda x: self.parse(x))\n",
        " \n",
        "    def parse(self, row):\n",
        "      return self.index_encoder[np.array(list(map(int, row.split(','))))] # adaptive softmax requires labels to be sorted by decreasing frequency\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        seq =  self.data['tracks'].iloc[index]\n",
        "        X = np.zeros(seq_len, dtype=np.int_)\n",
        "        l = len(seq)\n",
        "        if l < seq_len:\n",
        "          X[:l] = np.array(seq)\n",
        "        else :\n",
        "          r = random.randint(0, l-seq_len)\n",
        "          X = seq[r: r+seq_len] \n",
        "        return X\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPY3if3HTiAg"
      },
      "source": [
        "# Test : for each row, split text, convert to int, select 5 first tracks. Predict following tracks\n",
        "class SequentialTestDataset(Dataset):    \n",
        "    def __init__(self, filename, test_indices, sample_size, index_encoder):\n",
        "        self.sample_size = sample_size\n",
        "        self.index_encoder = index_encoder\n",
        "        self.data = pd.read_csv(filename, delimiter ='\\t',  header=None, names = ['tracks']).iloc[test_indices]\n",
        "        self.data['tracks'] = self.data['tracks'].apply(lambda x: self.parse(x))\n",
        "    def parse(self, row):\n",
        "      return self.index_encoder[np.array(list(map(int, row.split(','))))][:self.sample_size] # adaptive softmax requires labels to be sorted by decreasing frequency\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        X =  np.array(self.data[\"tracks\"].iloc[index])\n",
        "        return X\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiEqrHwwzlnd"
      },
      "source": [
        "# iterate over training set\n",
        "sequential_data_file = \"data/processed_data/playlist_sequence.csv\"\n",
        "\n",
        "sequential_train_dataset = SequentialTrainDataset(sequential_data_file, train_indices , seq_len, data_manager.tracks_rank)\n",
        "train_dataloader = DataLoader(sequential_train_dataset, batch_size = batch_size, shuffle=False)\n",
        "sequential_test_dataset = SequentialTestDataset(sequential_data_file, val_indices , seq_len - 1, data_manager.tracks_rank)\n",
        "test_dataloader = DataLoader(sequential_test_dataset, batch_size = test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1RC6rMnnEmL"
      },
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "def train(model, train_loader, test_loader, optimizer, dev, ordered_tracks_array, evaluator, n_epoch=10, clip=False):\n",
        "    print('=========training=========')\n",
        "    start = time.time()\n",
        "    for epoch in range(n_epoch):\n",
        "        print('----epoch', epoch)\n",
        "        batch_ct = 0\n",
        "        model.train()\n",
        "\n",
        "        for X in train_loader:\n",
        "            X = X.long().to(dev)\n",
        "            pred, loss = model(X) # (bs, ans_size)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            if clip :\n",
        "              clip_grad_norm_(model.parameters(), max_norm=clip, norm_type=2)\n",
        "            optimizer.step()\n",
        "            if batch_ct % 100 == 0:\n",
        "                current_time = time.time()\n",
        "                print('loss: {:.4f}'.format(loss))\n",
        "                print('batch %d elapsed time %f seconds' % (batch_ct, current_time - start))\n",
        "            batch_ct +=1\n",
        "        r_prec, ndcg, click = test(model, test_loader, evaluator, ordered_tracks_array)\n",
        "        current_time = time.time()\n",
        "        print('current performance at epoch %d elapsed time %f seconds' % (epoch, current_time - start))\n",
        "        print('r-precision : %f' % r_prec)\n",
        "        print('ndcg : %f' % ndcg)\n",
        "        print('click : %f' % click)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR1yr8JIowbE"
      },
      "source": [
        "def test(model, test_loader, evaluator, ordered_tracks_array):\n",
        "    model.eval()\n",
        "    recos_CNN = np.zeros((evaluator.test_size, evaluator.n_recos))\n",
        "    current_batch = 0\n",
        "    for X in test_loader:\n",
        "      X = X.long().to(dev)\n",
        "      bs = X.shape[0]\n",
        "      pred = model(X)\n",
        "      pred = pred[:,1:]\n",
        "      coded_recos = torch.argsort(pred, dim=1, descending=True)[:,:evaluator.n_recos].to('cpu').long() + 1\n",
        "      recos_CNN[current_batch * test_loader.batch_size: current_batch * test_loader.batch_size + bs] = ordered_tracks_array.take(coded_recos)\n",
        "      current_batch+=1\n",
        "    r_prec = evaluator.compute_R_precision(recos_CNN)\n",
        "    ndcg = evaluator.compute_ndcg(recos_CNN)\n",
        "    click = evaluator.compute_all_click(recos_CNN)\n",
        "    return r_prec, ndcg, click\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8xEQiToOOep"
      },
      "source": [
        "# from https://github.com/jojonki/Gated-Convolutional-Networks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class GatedCNN(nn.Module):\n",
        "    '''\n",
        "        In : (N, sentence_len)\n",
        "        Out: (N, sentence_len, embd_size)\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 seq_len,\n",
        "                 vocab_size,\n",
        "                 embd_size,\n",
        "                 n_layers,\n",
        "                 kernel,\n",
        "                 padding,\n",
        "                 out_chs,\n",
        "                 hidden_chs,\n",
        "                 res_block_count,\n",
        "                 init_factors_path,\n",
        "                 cutoffs):\n",
        "        super(GatedCNN, self).__init__()\n",
        "        self.res_block_count = res_block_count\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(np.load(init_factors_path)[:,:embd_size]), freeze=False).float()\n",
        "\n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
        "        self.conv_0 = nn.Conv2d(1, out_chs, kernel, padding=padding)\n",
        "        self.b_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "        self.conv_gate_0 = nn.Conv2d(1, out_chs, kernel, padding=padding)\n",
        "        self.c_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "\n",
        "        self.down_conv = nn.ModuleList([nn.Conv2d(out_chs, hidden_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])\n",
        "        self.bottle_conv = nn.ModuleList([nn.Conv2d(hidden_chs, hidden_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])  # bottleneck here\n",
        "        self.up_conv = nn.ModuleList([nn.Conv2d(hidden_chs, out_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])\n",
        "\n",
        "        self.down_conv_gate = nn.ModuleList([nn.Conv2d(out_chs, hidden_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)]) # bottlenecking here\n",
        "        self.bottle_conv_gate = nn.ModuleList([nn.Conv2d(hidden_chs, hidden_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])\n",
        "        self.up_conv_gate = nn.ModuleList([nn.Conv2d(hidden_chs, out_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)]) # bottlenecking here\n",
        "\n",
        "        self.fc = nn.Linear(out_chs*(seq_len - 1), vocab_size)\n",
        "        self.adapt = nn.AdaptiveLogSoftmaxWithLoss(out_chs*(seq_len - 1), vocab_size, cutoffs, div_value=1.8)\n",
        "        self.b = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "        self.c = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, seq_len)\n",
        "\n",
        "        # Embedding\n",
        "        bs = x.size(0) # batch size\n",
        "        if self.training:\n",
        "          target = x[:,-1]\n",
        "          x = x[:,:-1]\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) # (bs, seq_len, embd_size)\n",
        "\n",
        "        # CNN\n",
        "        x = x.unsqueeze(1) # (bs, Cin, seq_len, embd_size), insert Channnel-In dim\n",
        "        \n",
        "\n",
        "        # Conv2d\n",
        "        #    Input : (bs, Cin,  Hin,  Win )\n",
        "        #    Output: (bs, Cout, Hout, Wout)\n",
        "        A = self.conv_0(x)      # (bs, Cout, seq_len, 1)\n",
        "        A += self.b_0.repeat(1, 1, seq_len, 1)\n",
        "        B = self.conv_gate_0(x) # (bs, Cout, seq_len, 1)\n",
        "        B += self.c_0.repeat(1, 1, seq_len, 1)\n",
        "        h = A * torch.sigmoid(B)    # (bs, Cout, seq_len, 1)\n",
        "        res_input = h # TODO this is h1 not h0\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            A = self.up_conv[i](self.bottle_conv[i](self.down_conv[i](h))) + self.b[i].repeat(1, 1, seq_len, 1)\n",
        "            B = self.up_conv_gate[i](self.bottle_conv_gate[i](self.down_conv_gate[i](h))) + self.c[i].repeat(1, 1, seq_len, 1)\n",
        "            h = A * torch.sigmoid(B) # (bs, Cout, seq_len, 1)\n",
        "            if i % self.res_block_count == 0: # size of each residual block\n",
        "                h += res_input\n",
        "                res_input = h\n",
        "\n",
        "        h = h.view(bs, -1) # (bs, Cout*seq_len)\n",
        "        if self.training:\n",
        "          return self.adapt(h, target)\n",
        "        else :\n",
        "          return self.adapt.log_prob(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvjTs2ut7WXE"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "\n",
        "clip = 0.1\n",
        "gated_cnn = GatedCNN(seq_len, data_manager.n_tracks + 1, mid_dim, n_layers, kernel, padding, conv_size, hidden_size, res_block_count, item_factors_path, cutoffs ).to(dev)\n",
        "optimizer = torch.optim.SGD(gated_cnn.parameters(), lr=1.0, momentum = 0.1, nesterov=True)\n",
        "train(gated_cnn, train_dataloader, test_dataloader, optimizer, dev, data_manager.ordered_tracks, test_evaluator, clip=clip, n_epoch=10**12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owa4Qf2pUm5i",
        "outputId": "2662c7b3-144e-4572-8090-817bf3c7dd06"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "\n",
        "clip = 0.1\n",
        "gated_cnn = GatedCNN(seq_len, data_manager.n_tracks + 1, mid_dim, n_layers, kernel, padding, conv_size, hidden_size, res_block_count, item_factors_path, cutoffs ).to(dev)\n",
        "optimizer = torch.optim.SGD(gated_cnn.parameters(), lr=0.1, momentum = 0.1, nesterov=True)\n",
        "train(gated_cnn, train_dataloader, test_dataloader, optimizer, dev, data_manager.ordered_tracks, test_evaluator, clip=clip, n_epoch=80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========training=========\n",
            "----epoch 0\n",
            "loss: 15.4725\n",
            "batch 0 elapsed time 0.221540 seconds\n",
            "loss: 11.5455\n",
            "batch 1000 elapsed time 111.656628 seconds\n",
            "loss: 11.2690\n",
            "batch 2000 elapsed time 222.844069 seconds\n",
            "loss: 11.2320\n",
            "batch 3000 elapsed time 334.014528 seconds\n",
            "current performance at epoch 0 elapsed time 437.798936 seconds\n",
            "r-precision : 0.030713\n",
            "ndcg : 0.093735\n",
            "click : 16.916452\n",
            "----epoch 1\n",
            "loss: 11.2229\n",
            "batch 0 elapsed time 437.933214 seconds\n",
            "loss: 11.7884\n",
            "batch 1000 elapsed time 549.057018 seconds\n",
            "loss: 11.2956\n",
            "batch 2000 elapsed time 660.173142 seconds\n",
            "loss: 11.5965\n",
            "batch 3000 elapsed time 771.319351 seconds\n",
            "current performance at epoch 1 elapsed time 875.165737 seconds\n",
            "r-precision : 0.030689\n",
            "ndcg : 0.093260\n",
            "click : 16.771208\n",
            "----epoch 2\n",
            "loss: 11.3080\n",
            "batch 0 elapsed time 875.302786 seconds\n",
            "loss: 11.5669\n",
            "batch 1000 elapsed time 986.552961 seconds\n",
            "loss: 11.4021\n",
            "batch 2000 elapsed time 1097.762045 seconds\n",
            "loss: 11.5546\n",
            "batch 3000 elapsed time 1209.148337 seconds\n",
            "current performance at epoch 2 elapsed time 1313.295507 seconds\n",
            "r-precision : 0.030740\n",
            "ndcg : 0.093033\n",
            "click : 16.908740\n",
            "----epoch 3\n",
            "loss: 11.4543\n",
            "batch 0 elapsed time 1313.435967 seconds\n",
            "loss: 11.4945\n",
            "batch 1000 elapsed time 1425.003076 seconds\n",
            "loss: 11.2136\n",
            "batch 2000 elapsed time 1536.393068 seconds\n",
            "loss: 11.3239\n",
            "batch 3000 elapsed time 1647.809111 seconds\n",
            "current performance at epoch 3 elapsed time 1751.792390 seconds\n",
            "r-precision : 0.030776\n",
            "ndcg : 0.092594\n",
            "click : 16.853470\n",
            "----epoch 4\n",
            "loss: 11.1925\n",
            "batch 0 elapsed time 1751.931773 seconds\n",
            "loss: 11.9279\n",
            "batch 1000 elapsed time 1863.169839 seconds\n",
            "loss: 11.6445\n",
            "batch 2000 elapsed time 1974.273974 seconds\n",
            "loss: 11.6169\n",
            "batch 3000 elapsed time 2085.540490 seconds\n",
            "current performance at epoch 4 elapsed time 2189.456838 seconds\n",
            "r-precision : 0.030595\n",
            "ndcg : 0.093250\n",
            "click : 16.787918\n",
            "----epoch 5\n",
            "loss: 11.4750\n",
            "batch 0 elapsed time 2189.592196 seconds\n",
            "loss: 11.8013\n",
            "batch 1000 elapsed time 2300.614241 seconds\n",
            "loss: 11.4209\n",
            "batch 2000 elapsed time 2411.673412 seconds\n",
            "loss: 11.4076\n",
            "batch 3000 elapsed time 2522.856881 seconds\n",
            "current performance at epoch 5 elapsed time 2626.752184 seconds\n",
            "r-precision : 0.031083\n",
            "ndcg : 0.093077\n",
            "click : 16.901028\n",
            "----epoch 6\n",
            "loss: 11.5783\n",
            "batch 0 elapsed time 2626.889963 seconds\n",
            "loss: 11.4946\n",
            "batch 1000 elapsed time 2737.954334 seconds\n",
            "loss: 11.4043\n",
            "batch 2000 elapsed time 2848.997801 seconds\n",
            "loss: 11.2125\n",
            "batch 3000 elapsed time 2960.016063 seconds\n",
            "current performance at epoch 6 elapsed time 3063.642479 seconds\n",
            "r-precision : 0.030411\n",
            "ndcg : 0.093377\n",
            "click : 16.982005\n",
            "----epoch 7\n",
            "loss: 11.2798\n",
            "batch 0 elapsed time 3063.777645 seconds\n",
            "loss: 11.4307\n",
            "batch 1000 elapsed time 3174.778940 seconds\n",
            "loss: 11.2120\n",
            "batch 2000 elapsed time 3286.613205 seconds\n",
            "loss: 11.4386\n",
            "batch 3000 elapsed time 3397.842435 seconds\n",
            "current performance at epoch 7 elapsed time 3501.530646 seconds\n",
            "r-precision : 0.030306\n",
            "ndcg : 0.093017\n",
            "click : 16.915167\n",
            "----epoch 8\n",
            "loss: 11.5188\n",
            "batch 0 elapsed time 3501.665688 seconds\n",
            "loss: 11.5467\n",
            "batch 1000 elapsed time 3612.603266 seconds\n",
            "loss: 11.5005\n",
            "batch 2000 elapsed time 3723.672902 seconds\n",
            "loss: 11.4581\n",
            "batch 3000 elapsed time 3834.641797 seconds\n",
            "current performance at epoch 8 elapsed time 3938.358254 seconds\n",
            "r-precision : 0.030121\n",
            "ndcg : 0.092886\n",
            "click : 16.745501\n",
            "----epoch 9\n",
            "loss: 11.4908\n",
            "batch 0 elapsed time 3938.492593 seconds\n",
            "loss: 11.4090\n",
            "batch 1000 elapsed time 4049.519352 seconds\n",
            "loss: 11.1212\n",
            "batch 2000 elapsed time 4160.504751 seconds\n",
            "loss: 11.6624\n",
            "batch 3000 elapsed time 4271.527249 seconds\n",
            "current performance at epoch 9 elapsed time 4375.469162 seconds\n",
            "r-precision : 0.030019\n",
            "ndcg : 0.093067\n",
            "click : 16.908740\n",
            "----epoch 10\n",
            "loss: 11.2978\n",
            "batch 0 elapsed time 4375.602891 seconds\n",
            "loss: 11.4143\n",
            "batch 1000 elapsed time 4486.770927 seconds\n",
            "loss: 11.6782\n",
            "batch 2000 elapsed time 4597.878219 seconds\n",
            "loss: 11.5634\n",
            "batch 3000 elapsed time 4708.973496 seconds\n",
            "current performance at epoch 10 elapsed time 4812.853581 seconds\n",
            "r-precision : 0.030694\n",
            "ndcg : 0.093265\n",
            "click : 16.763496\n",
            "----epoch 11\n",
            "loss: 11.4210\n",
            "batch 0 elapsed time 4812.990227 seconds\n",
            "loss: 11.1439\n",
            "batch 1000 elapsed time 4924.245876 seconds\n",
            "loss: 11.4721\n",
            "batch 2000 elapsed time 5035.428971 seconds\n",
            "loss: 11.4806\n",
            "batch 3000 elapsed time 5146.783091 seconds\n",
            "current performance at epoch 11 elapsed time 5250.814696 seconds\n",
            "r-precision : 0.030380\n",
            "ndcg : 0.093566\n",
            "click : 16.888175\n",
            "----epoch 12\n",
            "loss: 11.2546\n",
            "batch 0 elapsed time 5250.952827 seconds\n",
            "loss: 11.6348\n",
            "batch 1000 elapsed time 5362.152857 seconds\n",
            "loss: 11.4893\n",
            "batch 2000 elapsed time 5473.266416 seconds\n",
            "loss: 11.5980\n",
            "batch 3000 elapsed time 5584.327861 seconds\n",
            "current performance at epoch 12 elapsed time 5688.069827 seconds\n",
            "r-precision : 0.030781\n",
            "ndcg : 0.093183\n",
            "click : 16.978149\n",
            "----epoch 13\n",
            "loss: 11.7096\n",
            "batch 0 elapsed time 5688.214861 seconds\n",
            "loss: 11.4270\n",
            "batch 1000 elapsed time 5799.381366 seconds\n",
            "loss: 11.0994\n",
            "batch 2000 elapsed time 5910.412677 seconds\n",
            "loss: 11.2817\n",
            "batch 3000 elapsed time 6021.420664 seconds\n",
            "current performance at epoch 13 elapsed time 6125.104420 seconds\n",
            "r-precision : 0.030539\n",
            "ndcg : 0.092701\n",
            "click : 16.970437\n",
            "----epoch 14\n",
            "loss: 11.5173\n",
            "batch 0 elapsed time 6125.237205 seconds\n",
            "loss: 11.4140\n",
            "batch 1000 elapsed time 6236.365354 seconds\n",
            "loss: 11.2838\n",
            "batch 2000 elapsed time 6347.372946 seconds\n",
            "loss: 11.2902\n",
            "batch 3000 elapsed time 6458.400763 seconds\n",
            "current performance at epoch 14 elapsed time 6562.286233 seconds\n",
            "r-precision : 0.030985\n",
            "ndcg : 0.093330\n",
            "click : 16.875321\n",
            "----epoch 15\n",
            "loss: 11.3296\n",
            "batch 0 elapsed time 6562.422694 seconds\n",
            "loss: 11.5364\n",
            "batch 1000 elapsed time 6673.527032 seconds\n",
            "loss: 11.5394\n",
            "batch 2000 elapsed time 6784.675244 seconds\n",
            "loss: 11.2970\n",
            "batch 3000 elapsed time 6895.710160 seconds\n",
            "current performance at epoch 15 elapsed time 6999.455699 seconds\n",
            "r-precision : 0.030690\n",
            "ndcg : 0.093764\n",
            "click : 16.934447\n",
            "----epoch 16\n",
            "loss: 11.5552\n",
            "batch 0 elapsed time 6999.593642 seconds\n",
            "loss: 11.7177\n",
            "batch 1000 elapsed time 7110.526229 seconds\n",
            "loss: 11.4987\n",
            "batch 2000 elapsed time 7221.692505 seconds\n",
            "loss: 11.6101\n",
            "batch 3000 elapsed time 7332.773250 seconds\n",
            "current performance at epoch 16 elapsed time 7436.606994 seconds\n",
            "r-precision : 0.031295\n",
            "ndcg : 0.093456\n",
            "click : 16.933162\n",
            "----epoch 17\n",
            "loss: 11.5382\n",
            "batch 0 elapsed time 7436.742123 seconds\n",
            "loss: 10.9602\n",
            "batch 1000 elapsed time 7547.904943 seconds\n",
            "loss: 11.3587\n",
            "batch 2000 elapsed time 7659.071820 seconds\n",
            "loss: 11.2844\n",
            "batch 3000 elapsed time 7770.184773 seconds\n",
            "current performance at epoch 17 elapsed time 7874.033850 seconds\n",
            "r-precision : 0.030609\n",
            "ndcg : 0.093520\n",
            "click : 17.039846\n",
            "----epoch 18\n",
            "loss: 11.3770\n",
            "batch 0 elapsed time 7874.169960 seconds\n",
            "loss: 11.2953\n",
            "batch 1000 elapsed time 7985.297019 seconds\n",
            "loss: 11.6174\n",
            "batch 2000 elapsed time 8096.407109 seconds\n",
            "loss: 11.6973\n",
            "batch 3000 elapsed time 8207.584965 seconds\n",
            "current performance at epoch 18 elapsed time 8311.422071 seconds\n",
            "r-precision : 0.030132\n",
            "ndcg : 0.093059\n",
            "click : 16.980720\n",
            "----epoch 19\n",
            "loss: 11.7342\n",
            "batch 0 elapsed time 8311.561548 seconds\n",
            "loss: 11.2129\n",
            "batch 1000 elapsed time 8422.691862 seconds\n",
            "loss: 11.4671\n",
            "batch 2000 elapsed time 8533.650413 seconds\n",
            "loss: 11.5042\n",
            "batch 3000 elapsed time 8644.605443 seconds\n",
            "current performance at epoch 19 elapsed time 8748.202507 seconds\n",
            "r-precision : 0.030232\n",
            "ndcg : 0.093484\n",
            "click : 16.901028\n",
            "----epoch 20\n",
            "loss: 11.2787\n",
            "batch 0 elapsed time 8748.342721 seconds\n",
            "loss: 11.3895\n",
            "batch 1000 elapsed time 8859.425965 seconds\n",
            "loss: 11.3878\n",
            "batch 2000 elapsed time 8970.239447 seconds\n",
            "loss: 11.4389\n",
            "batch 3000 elapsed time 9081.193668 seconds\n",
            "current performance at epoch 20 elapsed time 9184.800732 seconds\n",
            "r-precision : 0.030622\n",
            "ndcg : 0.093154\n",
            "click : 16.953728\n",
            "----epoch 21\n",
            "loss: 11.3133\n",
            "batch 0 elapsed time 9184.938159 seconds\n",
            "loss: 11.5555\n",
            "batch 1000 elapsed time 9295.892372 seconds\n",
            "loss: 11.4803\n",
            "batch 2000 elapsed time 9406.777292 seconds\n",
            "loss: 11.2390\n",
            "batch 3000 elapsed time 9517.789257 seconds\n",
            "current performance at epoch 21 elapsed time 9621.537691 seconds\n",
            "r-precision : 0.029523\n",
            "ndcg : 0.087876\n",
            "click : 16.691517\n",
            "----epoch 22\n",
            "loss: 11.3075\n",
            "batch 0 elapsed time 9621.679042 seconds\n",
            "loss: 11.2593\n",
            "batch 1000 elapsed time 9732.601828 seconds\n",
            "loss: 11.4927\n",
            "batch 2000 elapsed time 9843.598290 seconds\n",
            "loss: 11.3659\n",
            "batch 3000 elapsed time 9954.656231 seconds\n",
            "current performance at epoch 22 elapsed time 10058.619442 seconds\n",
            "r-precision : 0.029251\n",
            "ndcg : 0.085112\n",
            "click : 16.760925\n",
            "----epoch 23\n",
            "loss: 11.0129\n",
            "batch 0 elapsed time 10058.762842 seconds\n",
            "loss: 11.1208\n",
            "batch 1000 elapsed time 10170.061593 seconds\n",
            "loss: 11.2736\n",
            "batch 2000 elapsed time 10281.187006 seconds\n",
            "loss: 10.8654\n",
            "batch 3000 elapsed time 10392.412048 seconds\n",
            "current performance at epoch 23 elapsed time 10496.255459 seconds\n",
            "r-precision : 0.028542\n",
            "ndcg : 0.083697\n",
            "click : 16.695373\n",
            "----epoch 24\n",
            "loss: 11.2148\n",
            "batch 0 elapsed time 10496.393182 seconds\n",
            "loss: 11.0701\n",
            "batch 1000 elapsed time 10607.739879 seconds\n",
            "loss: 11.2639\n",
            "batch 2000 elapsed time 10718.732910 seconds\n",
            "loss: 11.2705\n",
            "batch 3000 elapsed time 10829.773540 seconds\n",
            "current performance at epoch 24 elapsed time 10933.464745 seconds\n",
            "r-precision : 0.026180\n",
            "ndcg : 0.080020\n",
            "click : 16.785347\n",
            "----epoch 25\n",
            "loss: 11.1424\n",
            "batch 0 elapsed time 10933.601528 seconds\n",
            "loss: 11.3536\n",
            "batch 1000 elapsed time 11044.610008 seconds\n",
            "loss: 11.2939\n",
            "batch 2000 elapsed time 11155.531953 seconds\n",
            "loss: 11.2717\n",
            "batch 3000 elapsed time 11266.459638 seconds\n",
            "current performance at epoch 25 elapsed time 11370.167635 seconds\n",
            "r-precision : 0.024584\n",
            "ndcg : 0.077205\n",
            "click : 17.562982\n",
            "----epoch 26\n",
            "loss: 11.2369\n",
            "batch 0 elapsed time 11370.300973 seconds\n",
            "loss: 11.6653\n",
            "batch 1000 elapsed time 11481.180639 seconds\n",
            "loss: 11.1580\n",
            "batch 2000 elapsed time 11592.135481 seconds\n",
            "loss: 11.0284\n",
            "batch 3000 elapsed time 11703.053282 seconds\n",
            "current performance at epoch 26 elapsed time 11806.681004 seconds\n",
            "r-precision : 0.022853\n",
            "ndcg : 0.073285\n",
            "click : 18.553985\n",
            "----epoch 27\n",
            "loss: 11.0943\n",
            "batch 0 elapsed time 11806.816312 seconds\n",
            "loss: 11.0912\n",
            "batch 1000 elapsed time 11917.699918 seconds\n",
            "loss: 11.5050\n",
            "batch 2000 elapsed time 12028.487762 seconds\n",
            "loss: 10.9757\n",
            "batch 3000 elapsed time 12139.380553 seconds\n",
            "current performance at epoch 27 elapsed time 12243.138363 seconds\n",
            "r-precision : 0.021504\n",
            "ndcg : 0.070114\n",
            "click : 19.660668\n",
            "----epoch 28\n",
            "loss: 11.0900\n",
            "batch 0 elapsed time 12243.273188 seconds\n",
            "loss: 11.3672\n",
            "batch 1000 elapsed time 12354.194877 seconds\n",
            "loss: 10.8608\n",
            "batch 2000 elapsed time 12465.183038 seconds\n",
            "loss: 11.0820\n",
            "batch 3000 elapsed time 12576.140121 seconds\n",
            "current performance at epoch 28 elapsed time 12679.782383 seconds\n",
            "r-precision : 0.020231\n",
            "ndcg : 0.065918\n",
            "click : 20.696658\n",
            "----epoch 29\n",
            "loss: 11.1640\n",
            "batch 0 elapsed time 12679.915524 seconds\n",
            "loss: 11.2195\n",
            "batch 1000 elapsed time 12790.887206 seconds\n",
            "loss: 10.8117\n",
            "batch 2000 elapsed time 12901.981964 seconds\n",
            "loss: 11.3427\n",
            "batch 3000 elapsed time 13012.878322 seconds\n",
            "current performance at epoch 29 elapsed time 13116.601256 seconds\n",
            "r-precision : 0.019895\n",
            "ndcg : 0.065569\n",
            "click : 21.065553\n",
            "----epoch 30\n",
            "loss: 10.9217\n",
            "batch 0 elapsed time 13116.735629 seconds\n",
            "loss: 11.2446\n",
            "batch 1000 elapsed time 13227.808115 seconds\n",
            "loss: 11.0895\n",
            "batch 2000 elapsed time 13338.843826 seconds\n",
            "loss: 10.7999\n",
            "batch 3000 elapsed time 13449.865993 seconds\n",
            "current performance at epoch 30 elapsed time 13553.717092 seconds\n",
            "r-precision : 0.020357\n",
            "ndcg : 0.064982\n",
            "click : 21.619537\n",
            "----epoch 31\n",
            "loss: 11.0198\n",
            "batch 0 elapsed time 13553.856446 seconds\n",
            "loss: 10.8429\n",
            "batch 1000 elapsed time 13665.250577 seconds\n",
            "loss: 11.1714\n",
            "batch 2000 elapsed time 13776.447953 seconds\n",
            "loss: 10.7798\n",
            "batch 3000 elapsed time 13887.562628 seconds\n",
            "current performance at epoch 31 elapsed time 13991.552718 seconds\n",
            "r-precision : 0.020108\n",
            "ndcg : 0.064247\n",
            "click : 22.574550\n",
            "----epoch 32\n",
            "loss: 10.8931\n",
            "batch 0 elapsed time 13991.687426 seconds\n",
            "loss: 11.2732\n",
            "batch 1000 elapsed time 14102.917834 seconds\n",
            "loss: 11.0604\n",
            "batch 2000 elapsed time 14213.991489 seconds\n",
            "loss: 11.1915\n",
            "batch 3000 elapsed time 14325.138253 seconds\n",
            "current performance at epoch 32 elapsed time 14428.982739 seconds\n",
            "r-precision : 0.020408\n",
            "ndcg : 0.064044\n",
            "click : 23.062982\n",
            "----epoch 33\n",
            "loss: 10.8089\n",
            "batch 0 elapsed time 14429.116824 seconds\n",
            "loss: 10.9552\n",
            "batch 1000 elapsed time 14540.150949 seconds\n",
            "loss: 11.2662\n",
            "batch 2000 elapsed time 14651.163596 seconds\n",
            "loss: 10.9721\n",
            "batch 3000 elapsed time 14762.101470 seconds\n",
            "current performance at epoch 33 elapsed time 14865.788221 seconds\n",
            "r-precision : 0.019580\n",
            "ndcg : 0.061890\n",
            "click : 24.037275\n",
            "----epoch 34\n",
            "loss: 10.9526\n",
            "batch 0 elapsed time 14865.921511 seconds\n",
            "loss: 11.0681\n",
            "batch 1000 elapsed time 14976.876481 seconds\n",
            "loss: 11.1148\n",
            "batch 2000 elapsed time 15087.787523 seconds\n",
            "loss: 10.9034\n",
            "batch 3000 elapsed time 15198.870189 seconds\n",
            "current performance at epoch 34 elapsed time 15302.531680 seconds\n",
            "r-precision : 0.020300\n",
            "ndcg : 0.062962\n",
            "click : 23.611825\n",
            "----epoch 35\n",
            "loss: 10.4890\n",
            "batch 0 elapsed time 15302.664577 seconds\n",
            "loss: 10.8580\n",
            "batch 1000 elapsed time 15413.572650 seconds\n",
            "loss: 11.2512\n",
            "batch 2000 elapsed time 15524.452171 seconds\n",
            "loss: 11.1425\n",
            "batch 3000 elapsed time 15635.363269 seconds\n",
            "current performance at epoch 35 elapsed time 15739.002527 seconds\n",
            "r-precision : 0.019322\n",
            "ndcg : 0.061819\n",
            "click : 23.895887\n",
            "----epoch 36\n",
            "loss: 11.1571\n",
            "batch 0 elapsed time 15739.140582 seconds\n",
            "loss: 10.9674\n",
            "batch 1000 elapsed time 15850.038426 seconds\n",
            "loss: 11.0317\n",
            "batch 2000 elapsed time 15960.999028 seconds\n",
            "loss: 11.0697\n",
            "batch 3000 elapsed time 16072.103514 seconds\n",
            "current performance at epoch 36 elapsed time 16175.974449 seconds\n",
            "r-precision : 0.019776\n",
            "ndcg : 0.063235\n",
            "click : 23.191517\n",
            "----epoch 37\n",
            "loss: 11.1492\n",
            "batch 0 elapsed time 16176.110135 seconds\n",
            "loss: 11.0929\n",
            "batch 1000 elapsed time 16286.967458 seconds\n",
            "loss: 10.6773\n",
            "batch 2000 elapsed time 16397.805081 seconds\n",
            "loss: 11.2655\n",
            "batch 3000 elapsed time 16508.814373 seconds\n",
            "current performance at epoch 37 elapsed time 16612.468189 seconds\n",
            "r-precision : 0.020514\n",
            "ndcg : 0.065727\n",
            "click : 22.771208\n",
            "----epoch 38\n",
            "loss: 10.9739\n",
            "batch 0 elapsed time 16612.600710 seconds\n",
            "loss: 10.9528\n",
            "batch 1000 elapsed time 16723.504224 seconds\n",
            "loss: 10.8490\n",
            "batch 2000 elapsed time 16834.427852 seconds\n",
            "loss: 11.1351\n",
            "batch 3000 elapsed time 16945.294947 seconds\n",
            "current performance at epoch 38 elapsed time 17048.966699 seconds\n",
            "r-precision : 0.019318\n",
            "ndcg : 0.062788\n",
            "click : 22.915167\n",
            "----epoch 39\n",
            "loss: 10.9859\n",
            "batch 0 elapsed time 17049.103535 seconds\n",
            "loss: 10.8418\n",
            "batch 1000 elapsed time 17160.131088 seconds\n",
            "loss: 10.7869\n",
            "batch 2000 elapsed time 17271.016322 seconds\n",
            "loss: 10.8510\n",
            "batch 3000 elapsed time 17381.974752 seconds\n",
            "current performance at epoch 39 elapsed time 17485.723886 seconds\n",
            "r-precision : 0.018229\n",
            "ndcg : 0.060705\n",
            "click : 23.633676\n",
            "----epoch 40\n",
            "loss: 10.8134\n",
            "batch 0 elapsed time 17485.859335 seconds\n",
            "loss: 10.9120\n",
            "batch 1000 elapsed time 17596.901463 seconds\n",
            "loss: 10.9944\n",
            "batch 2000 elapsed time 17707.932389 seconds\n",
            "loss: 11.1308\n",
            "batch 3000 elapsed time 17818.930481 seconds\n",
            "current performance at epoch 40 elapsed time 17922.747719 seconds\n",
            "r-precision : 0.017875\n",
            "ndcg : 0.059307\n",
            "click : 23.924165\n",
            "----epoch 41\n",
            "loss: 10.8562\n",
            "batch 0 elapsed time 17922.880637 seconds\n",
            "loss: 10.9486\n",
            "batch 1000 elapsed time 18033.861884 seconds\n",
            "loss: 11.0292\n",
            "batch 2000 elapsed time 18144.889264 seconds\n",
            "loss: 10.9109\n",
            "batch 3000 elapsed time 18255.939728 seconds\n",
            "current performance at epoch 41 elapsed time 18359.645740 seconds\n",
            "r-precision : 0.017718\n",
            "ndcg : 0.059112\n",
            "click : 24.146530\n",
            "----epoch 42\n",
            "loss: 10.9439\n",
            "batch 0 elapsed time 18359.781128 seconds\n",
            "loss: 10.9101\n",
            "batch 1000 elapsed time 18470.828834 seconds\n",
            "loss: 10.8294\n",
            "batch 2000 elapsed time 18581.884639 seconds\n",
            "loss: 11.1044\n",
            "batch 3000 elapsed time 18693.013796 seconds\n",
            "current performance at epoch 42 elapsed time 18796.885028 seconds\n",
            "r-precision : 0.017303\n",
            "ndcg : 0.057034\n",
            "click : 24.668380\n",
            "----epoch 43\n",
            "loss: 10.7834\n",
            "batch 0 elapsed time 18797.019949 seconds\n",
            "loss: 10.7307\n",
            "batch 1000 elapsed time 18908.127556 seconds\n",
            "loss: 11.2069\n",
            "batch 2000 elapsed time 19019.225015 seconds\n",
            "loss: 10.6485\n",
            "batch 3000 elapsed time 19130.361350 seconds\n",
            "current performance at epoch 43 elapsed time 19234.178573 seconds\n",
            "r-precision : 0.017372\n",
            "ndcg : 0.057809\n",
            "click : 24.888175\n",
            "----epoch 44\n",
            "loss: 10.8587\n",
            "batch 0 elapsed time 19234.316314 seconds\n",
            "loss: 10.9968\n",
            "batch 1000 elapsed time 19345.455303 seconds\n",
            "loss: 10.6064\n",
            "batch 2000 elapsed time 19456.603782 seconds\n",
            "loss: 10.9309\n",
            "batch 3000 elapsed time 19567.681154 seconds\n",
            "current performance at epoch 44 elapsed time 19671.527196 seconds\n",
            "r-precision : 0.016273\n",
            "ndcg : 0.055520\n",
            "click : 25.362468\n",
            "----epoch 45\n",
            "loss: 11.2157\n",
            "batch 0 elapsed time 19671.668057 seconds\n",
            "loss: 10.8907\n",
            "batch 1000 elapsed time 19782.827655 seconds\n",
            "loss: 11.1934\n",
            "batch 2000 elapsed time 19894.029518 seconds\n",
            "loss: 10.9903\n",
            "batch 3000 elapsed time 20005.091198 seconds\n",
            "current performance at epoch 45 elapsed time 20108.933534 seconds\n",
            "r-precision : 0.016544\n",
            "ndcg : 0.055187\n",
            "click : 26.168380\n",
            "----epoch 46\n",
            "loss: 11.0652\n",
            "batch 0 elapsed time 20109.071192 seconds\n",
            "loss: 11.1231\n",
            "batch 1000 elapsed time 20220.233364 seconds\n",
            "loss: 10.9189\n",
            "batch 2000 elapsed time 20331.356086 seconds\n",
            "loss: 11.1325\n",
            "batch 3000 elapsed time 20442.449043 seconds\n",
            "current performance at epoch 46 elapsed time 20546.231793 seconds\n",
            "r-precision : 0.015724\n",
            "ndcg : 0.054454\n",
            "click : 26.249357\n",
            "----epoch 47\n",
            "loss: 10.9262\n",
            "batch 0 elapsed time 20546.369447 seconds\n",
            "loss: 11.1709\n",
            "batch 1000 elapsed time 20657.484725 seconds\n",
            "loss: 10.9091\n",
            "batch 2000 elapsed time 20768.525115 seconds\n",
            "loss: 11.1225\n",
            "batch 3000 elapsed time 20879.709874 seconds\n",
            "current performance at epoch 47 elapsed time 20983.567458 seconds\n",
            "r-precision : 0.015844\n",
            "ndcg : 0.054336\n",
            "click : 26.474293\n",
            "----epoch 48\n",
            "loss: 10.9187\n",
            "batch 0 elapsed time 20983.705656 seconds\n",
            "loss: 10.7919\n",
            "batch 1000 elapsed time 21094.848414 seconds\n",
            "loss: 10.7382\n",
            "batch 2000 elapsed time 21205.960558 seconds\n",
            "loss: 11.0461\n",
            "batch 3000 elapsed time 21317.044285 seconds\n",
            "current performance at epoch 48 elapsed time 21420.778367 seconds\n",
            "r-precision : 0.016107\n",
            "ndcg : 0.053656\n",
            "click : 26.655527\n",
            "----epoch 49\n",
            "loss: 10.7661\n",
            "batch 0 elapsed time 21420.916522 seconds\n",
            "loss: 11.0378\n",
            "batch 1000 elapsed time 21532.080431 seconds\n",
            "loss: 10.9601\n",
            "batch 2000 elapsed time 21643.076642 seconds\n",
            "loss: 10.9630\n",
            "batch 3000 elapsed time 21754.041562 seconds\n",
            "current performance at epoch 49 elapsed time 21857.732016 seconds\n",
            "r-precision : 0.015485\n",
            "ndcg : 0.052856\n",
            "click : 27.271208\n",
            "----epoch 50\n",
            "loss: 10.7613\n",
            "batch 0 elapsed time 21857.864427 seconds\n",
            "loss: 10.9147\n",
            "batch 1000 elapsed time 21968.816610 seconds\n",
            "loss: 11.0442\n",
            "batch 2000 elapsed time 22079.748548 seconds\n",
            "loss: 11.0347\n",
            "batch 3000 elapsed time 22190.711634 seconds\n",
            "current performance at epoch 50 elapsed time 22294.357127 seconds\n",
            "r-precision : 0.015471\n",
            "ndcg : 0.051183\n",
            "click : 27.769923\n",
            "----epoch 51\n",
            "loss: 10.9613\n",
            "batch 0 elapsed time 22294.495490 seconds\n",
            "loss: 10.8563\n",
            "batch 1000 elapsed time 22405.424701 seconds\n",
            "loss: 10.5634\n",
            "batch 2000 elapsed time 22516.339509 seconds\n",
            "loss: 10.8886\n",
            "batch 3000 elapsed time 22627.189798 seconds\n",
            "current performance at epoch 51 elapsed time 22730.839606 seconds\n",
            "r-precision : 0.014867\n",
            "ndcg : 0.051384\n",
            "click : 27.746787\n",
            "----epoch 52\n",
            "loss: 10.8913\n",
            "batch 0 elapsed time 22730.977532 seconds\n",
            "loss: 10.5844\n",
            "batch 1000 elapsed time 22841.848974 seconds\n",
            "loss: 10.8569\n",
            "batch 2000 elapsed time 22952.727743 seconds\n",
            "loss: 10.6582\n",
            "batch 3000 elapsed time 23063.631308 seconds\n",
            "current performance at epoch 52 elapsed time 23167.378756 seconds\n",
            "r-precision : 0.015020\n",
            "ndcg : 0.051370\n",
            "click : 28.079692\n",
            "----epoch 53\n",
            "loss: 10.5302\n",
            "batch 0 elapsed time 23167.514032 seconds\n",
            "loss: 10.5814\n",
            "batch 1000 elapsed time 23278.660137 seconds\n",
            "loss: 10.6055\n",
            "batch 2000 elapsed time 23389.739414 seconds\n",
            "loss: 10.5459\n",
            "batch 3000 elapsed time 23500.644538 seconds\n",
            "current performance at epoch 53 elapsed time 23604.380601 seconds\n",
            "r-precision : 0.015337\n",
            "ndcg : 0.050951\n",
            "click : 28.111825\n",
            "----epoch 54\n",
            "loss: 10.6198\n",
            "batch 0 elapsed time 23604.514336 seconds\n",
            "loss: 10.7565\n",
            "batch 1000 elapsed time 23715.433719 seconds\n",
            "loss: 10.6338\n",
            "batch 2000 elapsed time 23826.524858 seconds\n",
            "loss: 10.7778\n",
            "batch 3000 elapsed time 23937.496714 seconds\n",
            "current performance at epoch 54 elapsed time 24041.169444 seconds\n",
            "r-precision : 0.014423\n",
            "ndcg : 0.048685\n",
            "click : 28.742931\n",
            "----epoch 55\n",
            "loss: 10.9945\n",
            "batch 0 elapsed time 24041.305467 seconds\n",
            "loss: 10.7199\n",
            "batch 1000 elapsed time 24152.347747 seconds\n",
            "loss: 10.5676\n",
            "batch 2000 elapsed time 24263.280981 seconds\n",
            "loss: 10.6736\n",
            "batch 3000 elapsed time 24374.318696 seconds\n",
            "current performance at epoch 55 elapsed time 24477.997753 seconds\n",
            "r-precision : 0.014195\n",
            "ndcg : 0.048171\n",
            "click : 28.791774\n",
            "----epoch 56\n",
            "loss: 10.1293\n",
            "batch 0 elapsed time 24478.130900 seconds\n",
            "loss: 10.4279\n",
            "batch 1000 elapsed time 24589.018544 seconds\n",
            "loss: 10.9795\n",
            "batch 2000 elapsed time 24700.015714 seconds\n",
            "loss: 10.9982\n",
            "batch 3000 elapsed time 24810.908329 seconds\n",
            "current performance at epoch 56 elapsed time 24914.688297 seconds\n",
            "r-precision : 0.014591\n",
            "ndcg : 0.051101\n",
            "click : 28.017995\n",
            "----epoch 57\n",
            "loss: 10.7862\n",
            "batch 0 elapsed time 24914.826015 seconds\n",
            "loss: 10.7366\n",
            "batch 1000 elapsed time 25025.778887 seconds\n",
            "loss: 10.9558\n",
            "batch 2000 elapsed time 25136.650060 seconds\n",
            "loss: 11.0631\n",
            "batch 3000 elapsed time 25247.634206 seconds\n",
            "current performance at epoch 57 elapsed time 25351.275350 seconds\n",
            "r-precision : 0.014415\n",
            "ndcg : 0.050300\n",
            "click : 28.578406\n",
            "----epoch 58\n",
            "loss: 11.0052\n",
            "batch 0 elapsed time 25351.410560 seconds\n",
            "loss: 10.5382\n",
            "batch 1000 elapsed time 25462.364240 seconds\n",
            "loss: 10.5459\n",
            "batch 2000 elapsed time 25573.297380 seconds\n",
            "loss: 10.3772\n",
            "batch 3000 elapsed time 25684.262596 seconds\n",
            "current performance at epoch 58 elapsed time 25787.905714 seconds\n",
            "r-precision : 0.013222\n",
            "ndcg : 0.046627\n",
            "click : 29.494859\n",
            "----epoch 59\n",
            "loss: 10.8727\n",
            "batch 0 elapsed time 25788.044071 seconds\n",
            "loss: 10.5048\n",
            "batch 1000 elapsed time 25898.994077 seconds\n",
            "loss: 11.0128\n",
            "batch 2000 elapsed time 26009.997051 seconds\n",
            "loss: 10.5566\n",
            "batch 3000 elapsed time 26120.963017 seconds\n",
            "current performance at epoch 59 elapsed time 26224.742448 seconds\n",
            "r-precision : 0.014502\n",
            "ndcg : 0.048615\n",
            "click : 28.861183\n",
            "----epoch 60\n",
            "loss: 10.7264\n",
            "batch 0 elapsed time 26224.880827 seconds\n",
            "loss: 10.3985\n",
            "batch 1000 elapsed time 26335.792943 seconds\n",
            "loss: 10.4968\n",
            "batch 2000 elapsed time 26446.636946 seconds\n",
            "loss: 10.2648\n",
            "batch 3000 elapsed time 26557.649477 seconds\n",
            "current performance at epoch 60 elapsed time 26661.390786 seconds\n",
            "r-precision : 0.014531\n",
            "ndcg : 0.049870\n",
            "click : 28.483290\n",
            "----epoch 61\n",
            "loss: 10.7201\n",
            "batch 0 elapsed time 26661.528490 seconds\n",
            "loss: 10.6748\n",
            "batch 1000 elapsed time 26772.389469 seconds\n",
            "loss: 10.7073\n",
            "batch 2000 elapsed time 26883.292511 seconds\n",
            "loss: 10.7187\n",
            "batch 3000 elapsed time 26994.233914 seconds\n",
            "current performance at epoch 61 elapsed time 27097.920710 seconds\n",
            "r-precision : 0.014347\n",
            "ndcg : 0.047343\n",
            "click : 28.960154\n",
            "----epoch 62\n",
            "loss: 10.8194\n",
            "batch 0 elapsed time 27098.055406 seconds\n",
            "loss: 10.3210\n",
            "batch 1000 elapsed time 27209.012695 seconds\n",
            "loss: 10.5445\n",
            "batch 2000 elapsed time 27320.008526 seconds\n",
            "loss: 10.4938\n",
            "batch 3000 elapsed time 27431.055548 seconds\n",
            "current performance at epoch 62 elapsed time 27534.903884 seconds\n",
            "r-precision : 0.014567\n",
            "ndcg : 0.047522\n",
            "click : 29.206941\n",
            "----epoch 63\n",
            "loss: 10.6075\n",
            "batch 0 elapsed time 27535.044478 seconds\n",
            "loss: 10.4286\n",
            "batch 1000 elapsed time 27646.088888 seconds\n",
            "loss: 10.6827\n",
            "batch 2000 elapsed time 27757.097327 seconds\n",
            "loss: 10.6646\n",
            "batch 3000 elapsed time 27868.206342 seconds\n",
            "current performance at epoch 63 elapsed time 27972.040010 seconds\n",
            "r-precision : 0.015073\n",
            "ndcg : 0.048602\n",
            "click : 28.937018\n",
            "----epoch 64\n",
            "loss: 10.8348\n",
            "batch 0 elapsed time 27972.179871 seconds\n",
            "loss: 10.3911\n",
            "batch 1000 elapsed time 28083.285626 seconds\n",
            "loss: 10.7416\n",
            "batch 2000 elapsed time 28194.414964 seconds\n",
            "loss: 10.9305\n",
            "batch 3000 elapsed time 28305.487947 seconds\n",
            "current performance at epoch 64 elapsed time 28409.311166 seconds\n",
            "r-precision : 0.014904\n",
            "ndcg : 0.048409\n",
            "click : 29.043702\n",
            "----epoch 65\n",
            "loss: 11.0215\n",
            "batch 0 elapsed time 28409.450796 seconds\n",
            "loss: 10.3248\n",
            "batch 1000 elapsed time 28520.583045 seconds\n",
            "loss: 10.4508\n",
            "batch 2000 elapsed time 28631.683327 seconds\n",
            "loss: 10.9678\n",
            "batch 3000 elapsed time 28742.804383 seconds\n",
            "current performance at epoch 65 elapsed time 28846.618185 seconds\n",
            "r-precision : 0.014900\n",
            "ndcg : 0.047562\n",
            "click : 29.051414\n",
            "----epoch 66\n",
            "loss: 10.4203\n",
            "batch 0 elapsed time 28846.753132 seconds\n",
            "loss: 10.4796\n",
            "batch 1000 elapsed time 28957.910642 seconds\n",
            "loss: 10.2459\n",
            "batch 2000 elapsed time 29069.012244 seconds\n",
            "loss: 10.3565\n",
            "batch 3000 elapsed time 29180.185211 seconds\n",
            "current performance at epoch 66 elapsed time 29284.037338 seconds\n",
            "r-precision : 0.014802\n",
            "ndcg : 0.046926\n",
            "click : 29.571979\n",
            "----epoch 67\n",
            "loss: 10.3874\n",
            "batch 0 elapsed time 29284.175613 seconds\n",
            "loss: 10.4263\n",
            "batch 1000 elapsed time 29395.231247 seconds\n",
            "loss: 10.5243\n",
            "batch 2000 elapsed time 29506.275555 seconds\n",
            "loss: 10.3304\n",
            "batch 3000 elapsed time 29617.382546 seconds\n",
            "current performance at epoch 67 elapsed time 29721.196661 seconds\n",
            "r-precision : 0.015247\n",
            "ndcg : 0.049160\n",
            "click : 28.838046\n",
            "----epoch 68\n",
            "loss: 10.1702\n",
            "batch 0 elapsed time 29721.333321 seconds\n",
            "loss: 10.6611\n",
            "batch 1000 elapsed time 29832.454129 seconds\n",
            "loss: 10.7196\n",
            "batch 2000 elapsed time 29943.677548 seconds\n",
            "loss: 10.7152\n",
            "batch 3000 elapsed time 30054.777106 seconds\n",
            "current performance at epoch 68 elapsed time 30158.611469 seconds\n",
            "r-precision : 0.014687\n",
            "ndcg : 0.048104\n",
            "click : 28.892031\n",
            "----epoch 69\n",
            "loss: 10.6289\n",
            "batch 0 elapsed time 30158.747208 seconds\n",
            "loss: 10.3054\n",
            "batch 1000 elapsed time 30269.902246 seconds\n",
            "loss: 10.9501\n",
            "batch 2000 elapsed time 30381.084542 seconds\n",
            "loss: 10.7385\n",
            "batch 3000 elapsed time 30492.169890 seconds\n",
            "current performance at epoch 69 elapsed time 30595.991078 seconds\n",
            "r-precision : 0.014214\n",
            "ndcg : 0.043994\n",
            "click : 30.728792\n",
            "----epoch 70\n",
            "loss: 10.6286\n",
            "batch 0 elapsed time 30596.134366 seconds\n",
            "loss: 10.4471\n",
            "batch 1000 elapsed time 30707.302802 seconds\n",
            "loss: 10.6428\n",
            "batch 2000 elapsed time 30818.343651 seconds\n",
            "loss: 10.4514\n",
            "batch 3000 elapsed time 30929.501261 seconds\n",
            "current performance at epoch 70 elapsed time 31033.388343 seconds\n",
            "r-precision : 0.015007\n",
            "ndcg : 0.048024\n",
            "click : 28.947301\n",
            "----epoch 71\n",
            "loss: 10.4979\n",
            "batch 0 elapsed time 31033.523751 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmdjGYtBi4pH"
      },
      "source": [
        "# from https://github.com/jojonki/Gated-Convolutional-Networks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class GatedCNN(nn.Module):\n",
        "    '''\n",
        "        In : (N, sentence_len)\n",
        "        Out: (N, sentence_len, embd_size)\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 seq_len,\n",
        "                 vocab_size,\n",
        "                 embd_size,\n",
        "                 n_layers,\n",
        "                 kernel,\n",
        "                 padding,\n",
        "                 out_chs,\n",
        "                 res_block_count,\n",
        "                 init_factors_path,\n",
        "                 cutoffs):\n",
        "        super(GatedCNN, self).__init__()\n",
        "        self.res_block_count = res_block_count\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(np.load(init_factors_path)[:,:embd_size]), freeze=False).float()\n",
        "\n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
        "        self.conv_0 = nn.Conv2d(1, out_chs, kernel, padding=padding)\n",
        "        self.b_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "        self.conv_gate_0 = nn.Conv2d(1, out_chs, kernel, padding=padding)\n",
        "        self.c_0 = nn.Parameter(torch.randn(1, out_chs, 1, 1))\n",
        "\n",
        "        self.conv = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])\n",
        "        self.conv_gate = nn.ModuleList([nn.Conv2d(out_chs, out_chs, (kernel[0], 1), padding=padding) for _ in range(n_layers)])\n",
        "        self.b = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "        self.c = nn.ParameterList([nn.Parameter(torch.randn(1, out_chs, 1, 1)) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(out_chs*(seq_len - 1), vocab_size)\n",
        "        self.adapt = nn.AdaptiveLogSoftmaxWithLoss(out_chs*(seq_len - 1), vocab_size, cutoffs, div_value=1.8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (N, seq_len)\n",
        "\n",
        "        # Embedding\n",
        "        bs = x.size(0) # batch size\n",
        "        if self.training:\n",
        "          target = x[:,-1]\n",
        "          x = x[:,:-1]\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) # (bs, seq_len, embd_size)\n",
        "\n",
        "        # CNN\n",
        "        x = x.unsqueeze(1) # (bs, Cin, seq_len, embd_size), insert Channnel-In dim\n",
        "        \n",
        "\n",
        "        # Conv2d\n",
        "        #    Input : (bs, Cin,  Hin,  Win )\n",
        "        #    Output: (bs, Cout, Hout, Wout)\n",
        "        A = self.conv_0(x)      # (bs, Cout, seq_len, 1)\n",
        "        A += self.b_0.repeat(1, 1, seq_len, 1)\n",
        "        B = self.conv_gate_0(x) # (bs, Cout, seq_len, 1)\n",
        "        B += self.c_0.repeat(1, 1, seq_len, 1)\n",
        "        h = A * torch.sigmoid(B)    # (bs, Cout, seq_len, 1)\n",
        "        res_input = h # TODO this is h1 not h0\n",
        "\n",
        "        for i, (conv, conv_gate) in enumerate(zip(self.conv, self.conv_gate)):\n",
        "            A = conv(h) + self.b[i].repeat(1, 1, seq_len, 1)\n",
        "            B = conv_gate(h) + self.c[i].repeat(1, 1, seq_len, 1)\n",
        "            h = A * torch.sigmoid(B) # (bs, Cout, seq_len, 1)\n",
        "            if i % self.res_block_count == 0: # size of each residual block\n",
        "                h += res_input\n",
        "                res_input = h\n",
        "\n",
        "        h = h.view(bs, -1) # (bs, Cout*seq_len)\n",
        "        if self.training:\n",
        "          return self.adapt(h, target)\n",
        "        else :\n",
        "          return self.adapt.log_prob(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bZw9d0ssQeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90c8b442-0384-439d-fdb1-c7e678f548b5"
      },
      "source": [
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"\n",
        "\n",
        "clip = 0.1\n",
        "gated_cnn = GatedCNN(seq_len, data_manager.n_tracks + 1, mid_dim, n_layers, kernel, padding, channels, res_block_count, item_factors_path, cutoffs ).to(dev)\n",
        "optimizer = torch.optim.SGD(gated_cnn.parameters(), lr=10.0, momentum = 0.1, nesterov=True)\n",
        "train(gated_cnn, train_dataloader, test_dataloader, optimizer, dev, data_manager.ordered_tracks, test_evaluator, clip=clip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========training=========\n",
            "----epoch 0\n",
            "loss: 15.2022\n",
            "batch 0 elapsed time 0.137938 seconds\n",
            "loss: 10.8786\n",
            "batch 1000 elapsed time 107.286998 seconds\n",
            "loss: 10.6975\n",
            "batch 2000 elapsed time 214.486134 seconds\n",
            "loss: 10.4046\n",
            "batch 3000 elapsed time 321.578002 seconds\n",
            "current performance at epoch 0 elapsed time 421.753333 seconds\n",
            "r-precision : 0.011551\n",
            "ndcg : 0.039929\n",
            "click : 31.848329\n",
            "----epoch 1\n",
            "loss: 10.2782\n",
            "batch 0 elapsed time 421.892482 seconds\n",
            "loss: 9.9826\n",
            "batch 1000 elapsed time 529.029066 seconds\n",
            "loss: 10.2310\n",
            "batch 2000 elapsed time 636.237832 seconds\n",
            "loss: 9.7470\n",
            "batch 3000 elapsed time 743.293571 seconds\n",
            "current performance at epoch 1 elapsed time 843.531823 seconds\n",
            "r-precision : 0.010043\n",
            "ndcg : 0.031696\n",
            "click : 34.926735\n",
            "----epoch 2\n",
            "loss: 9.6227\n",
            "batch 0 elapsed time 843.667435 seconds\n",
            "loss: 9.7500\n",
            "batch 1000 elapsed time 950.715488 seconds\n",
            "loss: 9.4346\n",
            "batch 2000 elapsed time 1057.779553 seconds\n",
            "loss: 9.8620\n",
            "batch 3000 elapsed time 1164.902343 seconds\n",
            "current performance at epoch 2 elapsed time 1265.122504 seconds\n",
            "r-precision : 0.009004\n",
            "ndcg : 0.029893\n",
            "click : 36.119537\n",
            "----epoch 3\n",
            "loss: 9.3239\n",
            "batch 0 elapsed time 1265.259208 seconds\n",
            "loss: 9.3373\n",
            "batch 1000 elapsed time 1372.314924 seconds\n",
            "loss: 9.4981\n",
            "batch 2000 elapsed time 1479.332998 seconds\n",
            "loss: 9.3956\n",
            "batch 3000 elapsed time 1586.306372 seconds\n",
            "current performance at epoch 3 elapsed time 1686.442863 seconds\n",
            "r-precision : 0.009635\n",
            "ndcg : 0.031291\n",
            "click : 34.852185\n",
            "----epoch 4\n",
            "loss: 9.3419\n",
            "batch 0 elapsed time 1686.582788 seconds\n",
            "loss: 9.6516\n",
            "batch 1000 elapsed time 1793.633979 seconds\n",
            "loss: 9.6765\n",
            "batch 2000 elapsed time 1900.540996 seconds\n",
            "loss: 9.0526\n",
            "batch 3000 elapsed time 2006.898602 seconds\n",
            "current performance at epoch 4 elapsed time 2106.492529 seconds\n",
            "r-precision : 0.009278\n",
            "ndcg : 0.029144\n",
            "click : 36.125964\n",
            "----epoch 5\n",
            "loss: 9.3762\n",
            "batch 0 elapsed time 2106.624875 seconds\n",
            "loss: 8.8937\n",
            "batch 1000 elapsed time 2212.831592 seconds\n",
            "loss: 9.3655\n",
            "batch 2000 elapsed time 2318.985278 seconds\n",
            "loss: 9.3897\n",
            "batch 3000 elapsed time 2425.198255 seconds\n",
            "current performance at epoch 5 elapsed time 2524.521199 seconds\n",
            "r-precision : 0.008124\n",
            "ndcg : 0.028594\n",
            "click : 36.205656\n",
            "----epoch 6\n",
            "loss: 9.3397\n",
            "batch 0 elapsed time 2524.653511 seconds\n",
            "loss: 8.7237\n",
            "batch 1000 elapsed time 2630.902600 seconds\n",
            "loss: 9.5611\n",
            "batch 2000 elapsed time 2737.124665 seconds\n",
            "loss: 9.4020\n",
            "batch 3000 elapsed time 2843.284153 seconds\n",
            "current performance at epoch 6 elapsed time 2942.675348 seconds\n",
            "r-precision : 0.009056\n",
            "ndcg : 0.031518\n",
            "click : 35.277635\n",
            "----epoch 7\n",
            "loss: 9.2727\n",
            "batch 0 elapsed time 2942.807975 seconds\n",
            "loss: 9.1980\n",
            "batch 1000 elapsed time 3049.073645 seconds\n",
            "loss: 9.3436\n",
            "batch 2000 elapsed time 3155.347295 seconds\n",
            "loss: 9.4126\n",
            "batch 3000 elapsed time 3261.508646 seconds\n",
            "current performance at epoch 7 elapsed time 3360.823658 seconds\n",
            "r-precision : 0.008930\n",
            "ndcg : 0.029250\n",
            "click : 35.926735\n",
            "----epoch 8\n",
            "loss: 9.2872\n",
            "batch 0 elapsed time 3360.956084 seconds\n",
            "loss: 9.1145\n",
            "batch 1000 elapsed time 3467.226458 seconds\n",
            "loss: 9.0639\n",
            "batch 2000 elapsed time 3573.456165 seconds\n",
            "loss: 8.8392\n",
            "batch 3000 elapsed time 3679.574110 seconds\n",
            "current performance at epoch 8 elapsed time 3778.883641 seconds\n",
            "r-precision : 0.008556\n",
            "ndcg : 0.030054\n",
            "click : 35.667095\n",
            "----epoch 9\n",
            "loss: 9.2969\n",
            "batch 0 elapsed time 3779.016182 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6156101eaa42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgated_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGatedCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tracks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_block_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_factors_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoffs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgated_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgated_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered_tracks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_evaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b5e9a626b7ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, dev, ordered_tracks_array, evaluator, n_epoch, clip)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m               \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_ct\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPUi2T3sp9fh"
      },
      "source": [
        "gated_cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhM567N978Oq"
      },
      "source": [
        "torch.save(gated_cnn, 'dummy_cnn')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}